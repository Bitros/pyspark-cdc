{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test in Azure Databricks \n",
    "- [Databricks Runtime 17.0](https://docs.databricks.com/aws/en/release-notes/runtime/17.0#change-delta-merge-python-and-scala-apis-to-return-dataframe-instead-of-unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_catalog_schema = \"\"\n",
    "test_external_location = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86383da7-2b7f-41de-b950-6d408d1d8893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import os\n",
    "os.environ[\"pyspark_cdc_test_catalog_schema\"] = f\"{test_catalog_schema}\"\n",
    "\n",
    "os.environ[\"pyspark_cdc_test_external_location\"] = f\"{test_external_location}\"\n",
    "\n",
    "# reload pyspark_cdc_test to make sure pick up the newest environment variables\n",
    "import importlib\n",
    "import pyspark_cdc_test\n",
    "importlib.reload(pyspark_cdc_test)\n",
    "from pyspark_cdc_test import external_location, catalog_schema\n",
    "print(external_location)\n",
    "print(catalog_schema)\n",
    "\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee\")\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee_mpk_dw\")\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee_mpk_iw\")\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee_spk_dw\")\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee_spk_dw_pz\")\n",
    "spark.sql(f\"drop table if exists {catalog_schema}.employee_spk_iw\")\n",
    "\n",
    "dbutils.fs.rm(external_location, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe5c5ab-da25-4bee-a710-983a2183fc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark_cdc_test import conftest\n",
    "from pyspark_cdc_test.watermark import test_watermark\n",
    "from pyspark_cdc_test.optimizer import test_cron_parser\n",
    "from pyspark_cdc_test.validator import test_validators\n",
    "from pyspark_cdc_test.capture import test_full_capture\n",
    "from pyspark_cdc_test.capture import test_incremental_capture\n",
    "\n",
    "\n",
    "pytest.main(\n",
    "    [\n",
    "        conftest.__file__,\n",
    "        test_watermark.__file__,\n",
    "        test_cron_parser.__file__,\n",
    "        test_validators.__file__,\n",
    "        test_full_capture.__file__,\n",
    "        test_incremental_capture.__file__,\n",
    "        \"--disable-warnings\",\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7132675548817829,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
